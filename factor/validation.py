"""
This script is designed for factor validation using linear regression.
"""
import datetime
import json
import os
import pathlib
import random
from collections import deque
from typing import Iterable

import numpy as np
import pandas as pd
from algo_engine.backtest import ProgressiveReplay
from algo_engine.base import MarketData
from quark.base import safe_exit, GlobalStatics
from quark.calibration.cross_validation import CrossValidation
from quark.calibration.dummies import is_market_session
from quark.calibration.kernel import SigmoidTransformer, BinaryTransformer
from quark.calibration.linear import *
from quark.datalore.utils import define_inputs, define_prediction, linear_decay_weights
from quark.factor import LOGGER, MDS, IndexWeight, FactorMonitor, ConcurrentMonitorManager, N_CORES, \
    SyntheticIndexMonitor
from quark.factor.decoder import RecursiveDecoder
from quark.factor.factor_pool import FACTOR_POOL, FactorPoolDummyMonitor
from quark.profile import cn
from api import postgres
from factor import simulated_env
from factor import helper

# from backtest import simulated_env
# from misc import helper

cn.profile_cn_override()
LOGGER = LOGGER.getChild('validation')
DUMMY_WEIGHT = 1
INDEX_WEIGHTS = IndexWeight(index_name='000016.SH')
TIME_ZONE = GlobalStatics.TIME_ZONE
RANGE_BREAK = GlobalStatics.RANGE_BREAK
START_DATE = datetime.date(2023, 1, 1)
END_DATE = datetime.date(2023, 4, 1)

if os.name == 'posix':
    MONITOR_MANAGER = ConcurrentMonitorManager(n_worker=N_CORES)
    MDS.monitor_manager = MONITOR_MANAGER
elif os.name == 'nt':
    LOGGER.warning(f'The program is running on {os.name} platform, concurrency is limited!')
    MONITOR_MANAGER = ConcurrentMonitorManager(n_worker=1)
    MDS.monitor_manager = MONITOR_MANAGER
else:
    raise OSError(f'The program is running on non-supported {os.name} platform!')


class FactorValidation(object):
    """
    Class for performing factor validation with replay and regression analysis.

    Attributes:
        validation_id (str): Identifier for the validation instance.
        subscription (list[str]): Market data subscription type.
        start_date (datetime.date): Start date for the replay.
        end_date (datetime.date): End date for the replay.
        sampling_interval (float): Interval for sampling market data.
        pred_target (str): Prediction target for validation.
        factor (list[FactorMonitor]): List of market data monitors for factor validation.
        factor_value (dict[float, dict[str, float]]): Dictionary to store validation metrics.

    Methods:
        __init__(self, **kwargs): Initialize the FactorValidation instance.
        initialize_factor(self, **kwargs) -> List[FactorMonitor]: Initialize multiple factors for validation.
        bod(self, market_date: datetime.date, replay: ProgressiveReplay, **kwargs) -> None: Execute beginning-of-day process.
        eod(self, market_date: datetime.date, replay: ProgressiveReplay, **kwargs) -> None: Execute end-of-day process.
        validation(self, market_date: datetime.date): Perform factor validation for the given market date.
        run(self): Run the factor validation process.
        reset(self): Reset the factor and factor_value data.

    Properties:
        features (list): Names of features for validation.
    """

    def __init__(self, **kwargs):
        """
        Initializes the FactorValidation instance.

        Args:
            **kwargs: Additional parameters for configuration.

        Keyword Args:
            dtype (str, optional): Type of market data. Defaults to 'TradeData'.
            start_date (datetime.date, optional): Start date for the replay. Defaults to START_DATE.
            end_date (datetime.date, optional): End date for the replay. Defaults to END_DATE.
            index_name (str, optional): Name of the index. Defaults to '000016.SH'.
            index_weights (dict, optional): Dictionary containing index weights. Defaults to INDEX_WEIGHTS.
            sampling_interval (float, optional): Interval for sampling market data. Defaults to 10.0.
            poly_degree (int, optional): Degree of polynomial features for regression analysis. Defaults to 1.
            pred_var (list, optional): List of prediction variables for validation. Defaults to ['target_actual'].
            decoder (object, optional): Decoder object for decoding predictions. Defaults to RecursiveDecoder(level=3).
            validation_id (str, optional): Identifier for the validation instance. Defaults to autogenerated ID.

        Note:
            The `index_weights` parameter should be provided as a dictionary where keys are ticker names and values are weights.
            The `decoder` parameter should be an object implementing decoding functionality for predictions.
            The `validation_id` parameter can be provided to specify a custom identifier for the validation instance.
        """

        # Params for replay
        self.dtype = kwargs.get('dtype', 'TradeData')
        self.start_date = kwargs.get('start_date', START_DATE)
        self.end_date = kwargs.get('end_date', END_DATE)
        self.market_date = kwargs.get('market_date', self.start_date)
        self.calendar = kwargs.get('calendar', None)

        # Params for index
        self.index_name = kwargs.get('index_name', '000016.SH')
        self.index_weights = kwargs.get('index_weights', INDEX_WEIGHTS)
        self._update_index_weights(market_date=self.start_date)
        self.subscription = list(self.index_weights.keys())

        # Params for factor sampling
        self.sampling_interval = kwargs.get('sampling_interval', 10.)

        # Params for validation
        self.poly_degree = kwargs.get('poly_degree', 1)
        self.pred_var = kwargs.get('pred_var', ['target_actual'])
        self.decoder = kwargs.get('decoder', RecursiveDecoder(level=3))
        self.pred_target = f'{self.index_name}.market_price'

        self.factor: list[FactorMonitor] = factor if isinstance(factor := kwargs.get('factor', []), list) else [factor]
        self.synthetic = SyntheticIndexMonitor(index_name=self.index_name, weights=self.index_weights,
                                               interval=self.sampling_interval)
        self.factor_value: dict[float, dict[str, float]] = {}
        self.factor_value_entry: dict[str, float] = {}

        self.model = {
            pred_var:
                QuantileLogRegression(l1=.01, l2=.01,
                                      transformer=SigmoidTransformer(lower_bound=0., upper_bound=0.02)) if pred_var in [
                    'up_actual', 'up_smoothed'] else
                QuantileLogRegression(l1=.01, l2=.01, transformer=SigmoidTransformer(lower_bound=-0.02,
                                                                                     upper_bound=0.)) if pred_var in [
                    'down_actual', 'down_smoothed'] else
                QuantileLogisticRegression(l1=.01, l2=.01,
                                           transformer=BinaryTransformer(center=0, scale=1.)) if pred_var in [
                    'state'] else
                QuantileRegularizedRegression(l1=.01, l2=.01)
            for pred_var in self.pred_var
        }
        self.cv = {pred_var: CrossValidation(model=model, folds=10, shuffle=True, strict_no_future=True) for
                   pred_var, model in self.model.items()}
        self.metrics = {pred_var: {} for pred_var in self.cv}
        self.validation_id = kwargs.get('validation_id', self._get_validation_id())
        self.dump_dir = f'{self.__class__.__name__}.{self.validation_id}'
        # this module is usually for debug usage
        # MONITOR_MANAGER.n_worker = 1

    def _get_validation_id(self):
        validation_id = 1

        while True:
            if os.path.isdir(dump_dir := f'{self.__class__.__name__}.{validation_id}'):
                validation_id += 1
            else:
                break

        return validation_id

    @classmethod
    def _cross_validation(cls, x, y, factors: pd.DataFrame, cv: CrossValidation):
        valid_mask = np.all(np.isfinite(x), axis=1) & np.isfinite(y)
        x = x[valid_mask]
        y = y[valid_mask]
        x_axis = np.array([datetime.datetime.fromtimestamp(_, tz=TIME_ZONE) for _ in factors.index])[valid_mask]

        cv.cross_validate(x=x, y=y)
        # for some cases, e.g. rolling forward cross validation, some x is not selected as x_val.
        # we need to a mask for those cases
        cv.x_axis = x_axis[-len(cv.x_val):]

    def _candle_sticks(self, factor_value: pd.DataFrame):
        import plotly.graph_objects as go

        candlestick_trace = go.Candlestick(
            name='Synthetic',
            x=[datetime.datetime.fromtimestamp(_, tz=TIME_ZONE) for _ in factor_value.index],
            open=factor_value[f'{self.synthetic.index_name}.open_price'],
            high=factor_value[f'{self.synthetic.index_name}.high_price'],
            low=factor_value[f'{self.synthetic.index_name}.low_price'],
            close=factor_value[f'{self.synthetic.index_name}.close_price'],
        )

        return candlestick_trace

    def _plot_cv(self, cv: CrossValidation, factors: pd.DataFrame, plot_wavelet: bool = True):
        import plotly.graph_objects as go
        fig = cv.plot()

        candlestick_trace = self._candle_sticks(factor_value=factors)
        candlestick_trace['yaxis'] = 'y3'
        fig.add_trace(candlestick_trace)

        if plot_wavelet:
            for level in range(self.decoder.level + 1):
                local_extreme = self.decoder.local_extremes(ticker=self.pred_target, level=level)

                if not local_extreme:
                    break

                y, x, wave_flag = zip(*local_extreme)
                x = [datetime.datetime.fromtimestamp(_, tz=TIME_ZONE) for _ in x]

                trace = go.Scatter(x=x, y=y, mode='lines', name=f'decode level {level}', yaxis='y3')
                fig.add_trace(trace)

        fig.update_xaxes(
            tickformat='%H:%M:%S',
            gridcolor='black',
            griddash='dash',
            minor_griddash="dot",
            showgrid=True,
            spikethickness=-2,
            rangebreaks=RANGE_BREAK,
            rangeslider_visible=False
        )

        fig.update_layout(
            yaxis3=dict(
                title="Synthetic",
                anchor="x",
                overlaying='y',
                side='right',
                showgrid=False
            )
        )

        return fig

    def _plot_factors(self, factors: pd.DataFrame, precision=4):
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots

        # Select relevant columns from factors
        features = self.features
        selected_factors = factors[features]
        hover_data = selected_factors.astype(np.float64)

        # Create subplot
        fig = make_subplots(
            rows=len(features) + 1,
            cols=1,
            shared_xaxes=False,
            subplot_titles=['Synthetic'] + features,
            row_heights=[3] + [1] * len(features)
        )

        candlestick_trace = self._candle_sticks(factor_value=factors)
        candlestick_trace['name'] = 'Synthetic'
        candlestick_trace['showlegend'] = True
        fig.add_trace(candlestick_trace, row=1, col=1)
        fig['layout'][f'yaxis']['title'] = 'Synthetic'

        # Add traces for each feature
        for i, feature in enumerate(features):
            trace = go.Scatter(
                x=candlestick_trace['x'],
                y=selected_factors[feature],
                mode='lines',
                name=feature,
                customdata=hover_data,
                hovertemplate='<br>'.join(
                    ['Datetime: %{x:%Y-%m-%d:%h}'] +
                    ['<b>' + feature + '</b><b>' + f": %{{y:.{precision}f}}" + '</b>'] +
                    [features[_] + f": %{{customdata[{_}]:.{precision}f}}" for _ in range(len(features)) if _ != i] +
                    ['<extra></extra>']  # otherwise another legend will be shown
                ),
                showlegend=True
            )

            fig.add_trace(trace, row=i + 2, col=1)
            # fig['layout'][f'yaxis{i + 2}']['title'] = feature
            fig.update_layout(
                {
                    f'yaxis{i + 2}': dict(
                        title=feature,
                        showgrid=True,
                        zeroline=True,
                        showticklabels=True,
                        showspikes=True,
                        # spikemode='across',
                        spikesnap='cursor',
                        spikethickness=-2,
                        # showline=False,
                        # spikedash='solid'
                    )
                }
            )

        fig.update_layout(
            title=dict(text="Factor Values for Synthetic"),
            height=200 * (3 + len(features)),
            template='simple_white',
            # legend_tracegroupgap=330,
            hovermode='x unified',
            legend_traceorder="normal"
        )

        fig.update_traces(xaxis=f'x1')

        fig.update_xaxes(
            tickformat='%H:%M:%S',
            gridcolor='black',
            griddash='dash',
            minor_griddash="dot",
            showgrid=True,
            spikethickness=-2,
            rangebreaks=RANGE_BREAK,
            rangeslider_visible=False
        )

        return fig

    def _update_index_weights(self, market_date: datetime.date):
        """
        Updates index weights based on the provided market date.

        Args:
            market_date (datetime.date): Date for which to update index weights.
        """
        index_weights = IndexWeight(
            index_name=self.index_name,
            **helper.load_dict(
                file_path=pathlib.Path(GlobalStatics.WORKING_DIRECTORY.value, 'Res',
                                       f'index_weights.{self.index_name}.{market_date:%Y%m%d}.json'),
                json_dict=simulated_env.query(ticker=self.index_name, market_date=market_date, topic='index_weights')
            )
        )

        # A lite setting for fast debugging
        if DUMMY_WEIGHT:
            for _ in list(index_weights.keys())[10:]:
                index_weights.pop(_)

        # Step 0: Update index weights
        self.index_weights.clear()
        self.index_weights.update(index_weights)
        self.index_weights.normalize()
        # the factors and synthetics are using the same index_weights reference, so no need to update individually
        # self.synthetic.weights = self.index_weights
        return index_weights

    def _update_subscription(self, replay: ProgressiveReplay):
        """
        Updates market data subscriptions based on index weights.
        """
        self.subscription.clear()
        replay.replay_subscription.clear()

        subscription = set(self.index_weights.keys())
        self.subscription.extend(subscription)

        if isinstance(self.dtype, str):
            dtype = [self.dtype]
        elif isinstance(self.dtype, Iterable):
            dtype = list(self.dtype)
        else:
            raise ValueError(f'Invalid dtype {self.dtype}')

        for ticker in subscription:
            for _dtype in dtype:
                replay.add_subscription(ticker=ticker, dtype=_dtype)

        # this should be only affecting behaviors of the concurrent manager using the shm feature.
        MDS.monitor_manager.subscription = subscription

    def _update_monitor_data(self, market_date: datetime.date):
        # for some factor using external data (like historical px data)
        # this update its data in this section
        pass

    def _update_factor_value(self, market_data: MarketData):
        idx = int(market_data.timestamp // self.sampling_interval)
        ts = idx * self.sampling_interval

        # update last log entry
        if ts not in self.factor_value:
            factor_value = MDS.monitor_manager.values
            self.factor_value_entry.update(factor_value)
            self.factor_value_entry = self.factor_value[ts] = {}

        self.factor_value_entry[f'{market_data.ticker}.market_price'] = market_data.market_price

    def initialize_factor(self, **kwargs) -> list[FactorMonitor]:
        """
        Initializes multiple factors for validation.

        Args:
            **kwargs: Additional parameters for factor configuration.

        Returns:
            list[MarketDataMonitor]: Initialized list of market data monitors.
        """
        if not self.factor:
            LOGGER.warning('No factor to be initialized! Is this intentional?')

        factors = self.factor.copy()

        self.factor.clear()

        for factor in factors:
            self.factor.append(
                factor.__class__(**factor.params)
            )

        for factor in self.factor:
            MDS.add_monitor(factor)

        MDS.add_monitor(self.synthetic)

        return self.factor

    def validation(self, market_date: datetime.date):
        """
        Performs factor validation for the given market date.

        Args:
            market_date (datetime.date): Current market date.
        """
        if not self.factor_value:
            return

        LOGGER.info(f'{market_date} validation started with {len(self.factor_value):,} obs.')

        factor_value = pd.DataFrame(self.factor_value).T
        print(factor_value)

        for pred_var in self.pred_var:
            cv = self.cv[pred_var]
            metrics = self.metrics[pred_var]

            # Step 1: define input and target
            x = define_inputs(factor_value=factor_value, input_vars=self.features,
                              poly_degree=self.poly_degree).to_numpy()
            y = define_prediction(factor_value=factor_value, pred_var=pred_var, decoder=self.decoder,
                                  key=self.pred_target)
            y = (y - np.nanmedian(y)).to_numpy()

            # Step 2: Regression analysis
            self._cross_validation(x=x, y=y, factors=factor_value, cv=cv)

            # Step 3: Log metrics
            metrics[market_date] = cv.metrics.metrics

    def dump_result(self, market_date: datetime.date):
        os.makedirs(self.dump_dir, exist_ok=True)

        factor_value = pd.DataFrame(self.factor_value).T

        entry_dir = pathlib.Path(self.dump_dir, f'{market_date:%Y-%m-%d}')
        os.makedirs(entry_dir, exist_ok=True)

        if len(self.factor) > 2:
            file_name = f'{self.__class__.__name__}'
        else:
            file_name = f'{"".join([f"[{factor.name}]" for factor in self.factor])}.validation'

        factor_value.to_csv(pathlib.Path(entry_dir, f'{file_name}.factors.csv'))

        fig = self._plot_factors(factors=factor_value)
        fig.write_html(pathlib.Path(entry_dir, f'{file_name}.factor.html'))

        for pred_var in self.pred_var:
            model = self.model[pred_var]
            cv = self.cv[pred_var]
            metrics = self.metrics[pred_var]

            if cv.x_val is not None:
                fig = self._plot_cv(cv=cv, factors=factor_value)
                fig.write_html(pathlib.Path(entry_dir, f'{file_name}.{pred_var}.pred.html'))

                model.dump(pathlib.Path(entry_dir, f'{file_name}.{pred_var}.model.json'))
                cv.metrics.to_html(pathlib.Path(entry_dir, f'{file_name}.{pred_var}.metrics.html'))
                pd.DataFrame(metrics).T.to_csv(pathlib.Path(self.dump_dir, f'metrics.{pred_var}.csv'))

    def reset(self):
        """
        Resets the factor and factor_value data.
        """
        for factor in self.factor:
            factor.clear()

        MDS.clear()

        self.factor_value.clear()
        self.decoder.clear()

        for cv in self.cv.values():
            cv.clear()

    def run(self):
        """
        Runs the factor validation process.
        """

        replay = ProgressiveReplay(
            loader=postgres.StockTransactionQuery().__call__,
            tickers=[],  # ticker is registered upon BoD function
            dtype=self.dtype.split(','),
            start_date=self.market_date,
            end_date=self.end_date,
            calendar=simulated_env.trade_calendar(start_date=self.start_date,
                                                  end_date=self.end_date) if self.calendar is None else self.calendar,
            bod=self.bod,
            eod=self.eod,
            tick_size=0.001,
        )

        for market_data in replay:  # type: MarketData
            if not is_market_session(market_data.timestamp):
                continue

            self._update_factor_value(market_data=market_data)
            MDS.on_market_data(market_data=market_data)

    def bod(self, market_date: datetime.date, replay: ProgressiveReplay, skip_init: bool = False, **kwargs) -> None:
        LOGGER.info(f'Starting {market_date} bod process...')

        self.market_date = market_date

        # Startup task 0: Update subscription
        self._update_index_weights(market_date=market_date)

        # backtest specific action 1: Unzip data
        # historical.unzip_batch(market_date=market_date, ticker_list=self.index_weights.keys())

        # Startup task 2: Update subscription and replay
        self._update_subscription(replay=replay)

        # Startup task 3: Update caches
        self.initialize_factor()

        # Startup task 4: update factor data
        self._update_monitor_data(market_date=market_date)

        # start the manager
        if not skip_init:
            MDS.monitor_manager.start()

    def eod(self, market_date: datetime.date, replay: ProgressiveReplay, **kwargs) -> None:
        random.seed(42)
        LOGGER.info(f'Starting {market_date} eod process...')

        # stop the manager
        MDS.monitor_manager.stop()

        self.validation(market_date=market_date)

        self.dump_result(market_date=market_date)

        self.reset()

    @property
    def features(self) -> list[str]:
        features = []

        for factor in self.factor:
            features.extend(factor.factor_names(subscription=list(self.index_weights)))

        return features


class FactorValidationCached(FactorValidation):

    def __init__(self, **kwargs):
        """
        Initializes the FactorBatchValidation instance.

        Args:
            **kwargs: Additional parameters for configuration.
        """
        super().__init__(
            poly_degree=kwargs.pop('poly_degree', 2),
            **kwargs
        )

        self.override_cache = kwargs.get('override_cache', False)
        self.cache_dir = None

        self.factor_pool = FACTOR_POOL
        self.factor_cache = FactorPoolDummyMonitor(factor_pool=self.factor_pool)

    def initialize_factor(self, **kwargs) -> list[FactorMonitor]:
        super().initialize_factor(**kwargs)

        if not self.override_cache:
            MDS.add_monitor(self.factor_cache)

        return self.factor

    def initialize_cache(self, market_date: datetime.date, replay: ProgressiveReplay, auto_skip: bool = True):
        if self.override_cache:
            return

        self.factor_pool.load(market_date=market_date, factor_dir=self.cache_dir)
        factor_existed = self.factor_pool.factor_names(market_date=market_date)

        for factor in self.factor:
            factor_names = factor.factor_names(subscription=list(self.subscription))

            if all([_ in factor_existed for _ in factor_names]):
                factor.enabled = False
                LOGGER.info(f'Factor {factor.name} found in the factor cache, and will be disabled.')

        # no replay task is needed, remove all tasks
        if auto_skip and all([not factor.enabled for factor in self.factor]):
            replay.replay_subscription.clear()
            self.subscription.clear()  # need to ensure the synchronization of the subscription
            LOGGER.info(f'{market_date} All factor is cached, skip this day.')
            self.factor_value.update(self.factor_pool.storage[market_date])

    def _update_cache(self, market_date: datetime):
        if self.override_cache:
            LOGGER.info(f'Cache {market_date} overridden!')
            self.factor_pool.batch_update(factors=self.factor_value)
        else:
            exclude_keys = self.factor_pool.factor_names(market_date=market_date)
            self.factor_pool.batch_update(factors=self.factor_value, exclude_keys=exclude_keys)

            if all([name in exclude_keys for name in pd.DataFrame(self.factor_value).T.columns]):
                return

            LOGGER.info('Cache updated!')

        self.factor_pool.dump(factor_dir=self.cache_dir)

    def bod(self, market_date: datetime.date, replay: ProgressiveReplay, **kwargs) -> None:
        super().bod(market_date=market_date, replay=replay, skip_init=True, **kwargs)

        self.initialize_cache(market_date=market_date, replay=replay)

        MDS.monitor_manager.start()

    def eod(self, market_date: datetime.date, **kwargs) -> None:
        self._update_cache(market_date=market_date)

        super().eod(market_date=market_date, **kwargs)

    def reset(self):
        super().reset()

        self.factor_cache.clear()


class InterTemporalValidation(FactorValidationCached):
    """
    model is trained prior to the beginning of the day, using multi-day data
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.training_days: int = kwargs.get('training_days', 5)
        self.factor_value_storage = deque(maxlen=self.training_days)

    @classmethod
    def _out_sample_validation(cls, x_train, y_train, weights, x_val, y_val, factors: pd.DataFrame,
                               cv: CrossValidation):
        x_axis = np.array([datetime.datetime.fromtimestamp(_, tz=TIME_ZONE) for _ in factors.index])

        valid_mask = np.all(np.isfinite(x_train), axis=1) & np.isfinite(y_train)
        x_train = x_train[valid_mask]
        y_train = y_train[valid_mask]
        weights = weights[valid_mask]

        valid_mask = np.all(np.isfinite(x_val), axis=1) & np.isfinite(y_val)
        x_val = x_val[valid_mask]
        y_val = y_val[valid_mask]
        x_axis = x_axis[valid_mask]

        # if isinstance(self.model, RidgeRegression):
        #     self.model.optimal_alpha(x=x_train, y=y_train)

        cv.validate(x_train=x_train, y_train=y_train, sample_weights=weights, x_val=x_val, y_val=y_val)
        cv.x_axis = x_axis

    def _split_test_set(self, pred_var: str, features: list[str],
                        factor_value_storage: list[dict[float, dict[str, float]]]):
        x_list, y_list, weights = [], [], []
        for i, factor_value in enumerate(factor_value_storage):
            factor_value = pd.DataFrame(factor_value).T
            self.decoder.clear()

            _x = define_inputs(factor_value=factor_value, input_vars=features, poly_degree=self.poly_degree).to_numpy()
            _y = define_prediction(factor_value=factor_value, pred_var=pred_var, decoder=self.decoder,
                                   key=self.pred_target)
            _y = (_y - np.nanmedian(_y)).to_numpy()
            _weights = linear_decay_weights(i=i, total=self.training_days)

            assert _weights > 0

            x_list.append(_x)
            y_list.append(_y)
            weights.append(np.ones(shape=_x.shape[0]) * _weights)  # the training weights is using linear decay

        if not x_list:
            x_train = y_train = x_val = y_val = None
        elif len(x_list) == 1:
            x_train = y_train = None
            x_val, y_val, weights = x_list[-1], y_list[-1], weights[-1]
        else:
            x_train = np.concatenate(x_list[:-1])
            y_train = np.concatenate(y_list[:-1])
            weights = np.concatenate(weights[:-1])
            x_val, y_val = x_list[-1], y_list[-1]  # validation set does not need weights

        return x_train, y_train, weights, x_val, y_val

    def validation(self, market_date: datetime.date):
        self.factor_value_storage.append(self.factor_value.copy())
        LOGGER.info(f'{market_date} validation started with {len(self.factor_value_storage):,} days obs.')

        factor_value = pd.DataFrame(self.factor_value).T

        for pred_var in self.pred_var:
            cv = self.cv[pred_var]
            metrics = self.metrics[pred_var]

            # Step 1: define input and target
            x_train, y_train, weights, x_val, y_val = self._split_test_set(
                pred_var=pred_var,
                features=self.features,
                factor_value_storage=list(self.factor_value_storage)
            )

            if x_train is None:
                return

            # Step 2: Cross validation
            self._out_sample_validation(
                x_train=x_train,
                y_train=y_train,
                weights=weights,
                x_val=x_val,
                y_val=y_val,
                factors=factor_value,
                cv=cv
            )

            # Step 3: Log metrics
            metrics[market_date] = cv.metrics.metrics


class FactorParamsOptimizer(InterTemporalValidation):
    """
    similar to .grid_cv of the sklearn
    factor = operator(params)
    monitor a list of the factor (same opterator with varied params),
    calculate metrics using cross validation
    select the best parameter and use in the next day
    update the factor cache with the factor with (approx-) optimal params
    """

    def __init__(self, parent_factor: FactorMonitor = None, **kwargs):
        self.parent_factor: FactorMonitor = parent_factor
        self.auxiliary_factor: list[FactorMonitor] = auxiliary_factor if isinstance(
            auxiliary_factor := kwargs.get('auxiliary_factor', []), list) else [auxiliary_factor]
        self.grid_factor: list[FactorMonitor] | None = kwargs.get('grid_factor', kwargs.pop('factor', None))

        super().__init__(
            cache_dir=kwargs.get('cache_dir',
                                 pathlib.Path(GlobalStatics.WORKING_DIRECTORY.value, 'Res', 'opt_factor_cache')),
            **kwargs
        )

        if parent_factor is None and self.grid_factor is None:
            raise ValueError('Must assign a parent_factor or grid_factor')
        elif self.grid_factor is None:
            self.grid_factor: list[FactorMonitor] = [self.parent_factor.__class__(**params) for params in
                                                     self.parent_factor.params_list()]
            LOGGER.info(f'Grid factor auto generated, {len(self.grid_factor)} factors added!')
        elif self.parent_factor is None:
            pass
        else:
            LOGGER.warning(f'{self.__class__} parent factor assigned and override by the grid_factors.')

        self.grid_model = {
            f'{factor.name}.{pred_var}':
                QuantileLogRegression(l1=.01, l2=.01,
                                      transformer=SigmoidTransformer(lower_bound=0., upper_bound=0.02)) if pred_var in [
                    'up_actual', 'up_smoothed'] else
                QuantileLogRegression(l1=.01, l2=.01, transformer=SigmoidTransformer(lower_bound=-0.02,
                                                                                     upper_bound=0.)) if pred_var in [
                    'down_actual', 'down_smoothed'] else
                QuantileLogisticRegression(l1=.01, l2=.01,
                                           transformer=BinaryTransformer(center=0, scale=1.)) if pred_var in [
                    'state'] else
                QuantileRegularizedRegression(l1=.01, l2=.01)
            for pred_var in self.pred_var
            for factor in self.grid_factor
        }

        self.grid_cv = {key: CrossValidation(model=model, folds=10, shuffle=True, strict_no_future=True) for key, model
                        in self.grid_model.items()}
        self.grid_metrics = {key: {} for key in self.grid_cv}  # metrics for each factor, for each target
        self.grid_avg_metrics = {factor.name: {} for factor in
                                 self.grid_factor}  # metrics for each factor, average of all targets
        self.grid_score: dict[str, dict[datetime.date, float]] = {factor.name: {} for factor in
                                                                  self.grid_factor}  # this is the score using for optimal factor selection
        self.grid_select_metrics = {pred_var: {} for pred_var in self.pred_var}
        self.grid_optimal_factor: FactorMonitor | None = None

    def initialize_factor(self, **kwargs) -> list[FactorMonitor]:
        # clear the factor for each day
        self.factor.clear()

        # initialize factor based on selected optimal params, this must come first
        if self.grid_optimal_factor is not None:
            constructor = self.grid_optimal_factor.__class__
            params = self.grid_optimal_factor.params
            selected_factor = constructor(**params)
            selected_factor.name = 'Monitor.GridCV.Selected'
            self.factor.append(selected_factor)
        # use parent factor as initial guess
        else:
            default_factor = self.grid_factor[0] if self.parent_factor is None else self.parent_factor
            selected_factor = default_factor.__class__(**default_factor.params)
            selected_factor.name = 'Monitor.GridCV.Selected'
            self.factor.append(selected_factor)

        # initialize grid factors
        self.factor.extend(self.grid_factor)

        # initialize auxiliary factors
        self.factor.extend(self.auxiliary_factor)

        super().initialize_factor(**kwargs)

        # for factor in self.factor:
        #     MDS.add_monitor(factor)
        #
        # MDS.add_monitor(self.synthetic)
        #
        # if not self.override_cache:
        #     MDS.add_monitor(self.factor_cache)

        return self.factor

    def validation(self, market_date: datetime.date):
        # remove grid factors from the self.factor
        optimal_factor, *factors = self.factor

        self.factor.clear()
        self.factor.append(optimal_factor)
        self.factor.extend(self.auxiliary_factor)

        # parent factor validation
        super().validation(market_date=market_date)

        # grid cv
        self.grid_validation(market_date=market_date)

        # select best score from cv
        self.select_best_params(market_date=market_date)

    def grid_validation(self, market_date: datetime.date):
        factor_value = pd.DataFrame(self.factor_value).T
        grid_metrics = {}

        for factor in self.grid_factor:
            factor_name = factor.name

            LOGGER.info(f'[Grid CV] Calculating score for {factor_name}...')
            grid_metrics[factor_name] = {}
            avg_metrics = self.grid_avg_metrics[factor_name]

            for pred_var in self.pred_var:
                key = f'{factor_name}.{pred_var}'
                grid_features = [feature for feature in factor.factor_names(list(self.index_weights))]
                auxiliary_features = [feature for auxiliary_factor in self.auxiliary_factor for feature in
                                      auxiliary_factor.factor_names(list(self.index_weights))]
                features = grid_features + auxiliary_features
                cv = self.grid_cv[key]
                metrics = self.grid_metrics[key]

                x_train, y_train, weights, x_val, y_val = self._split_test_set(
                    pred_var=pred_var,
                    features=features,
                    factor_value_storage=list(self.factor_value_storage)
                )

                if x_train is None:
                    continue

                self._out_sample_validation(
                    x_train=x_train,
                    y_train=y_train,
                    weights=weights,
                    x_val=x_val,
                    y_val=y_val,
                    factors=factor_value,
                    cv=cv
                )

                grid_metrics[factor_name][pred_var] = metrics[market_date] = cv.metrics.metrics

            all_metrics = {}

            if not grid_metrics:
                continue

            for _metrics in grid_metrics[factor_name].values():
                for name, value in _metrics.items():
                    if name in all_metrics:
                        entry = all_metrics[name]
                    else:
                        entry = all_metrics[name] = []

                    entry.append(value)

            avg_metrics[market_date] = {
                key: np.mean(valid_value) if (valid_value := [_v for _v in value if np.isfinite(_v)]) else np.nan for
                key, value in all_metrics.items()}

        return grid_metrics

    def select_best_params(self, market_date: datetime.date, alpha: float = 0.5):
        """
        calculate score and select best params for next day.
        note that the score is an ema of previous (validation) average metrics.
        Args:
            market_date:
            alpha:

        Returns:

        """
        grid_metrics = {}
        grid_score = {}
        last_selected = None if self.grid_optimal_factor is None else self.grid_optimal_factor.name

        for factor in self.grid_factor:
            factor_name = factor.name
            grid_metrics[factor_name] = {}

            for pred_var in self.pred_var:
                key = f'{factor_name}.{pred_var}'
                metrics = self.grid_metrics[key]

                if market_date not in metrics:
                    continue

                grid_metrics[factor_name][pred_var] = metrics[market_date]

                # before override the optimal params, must log the cv metrics result of the last selected factor
                if factor_name == last_selected:
                    self.grid_select_metrics[pred_var][market_date] = metrics[market_date]

            if not grid_metrics[factor_name]:
                continue

            score = float(self._grid_score(grid_metrics[factor_name]))
            self.grid_score[factor_name][market_date] = score
            # calculate a simple ema value using pandas, get the last ema value as the average score
            score_ema = pd.Series(list(self.grid_score[factor_name].values())).ewm(alpha=alpha).mean().values[-1]
            grid_score[factor_name] = score_ema

        if not grid_score:
            return None

        best_factor, best_score = sorted(iter(grid_score.items()), key=lambda x: x[1], reverse=True)[0]
        best_factor = [_ for _ in self.grid_factor if _.name == best_factor][0]

        LOGGER.info(
            f'Grid CV complete! Best params for {market_date} is {best_factor.name} with score {best_score:.4%}:\n{ {key: value for key, value in best_factor.params.items() if key != "weights"} }')

        self.grid_optimal_factor = best_factor
        return best_factor

    @classmethod
    def _grid_score(cls, grid_metrics: dict[str, dict[str, float]], keys=None):

        if keys is None:
            keys = ['Acc_Selection_AUC']

        scores = []
        for metrics in grid_metrics.values():

            if missing_keys := [key for key in keys if key not in metrics]:
                LOGGER.warning(f'Metric {missing_keys} not available, expect names in {metrics.keys()}')

            if not (available_keys := [key for key in keys if key in metrics]):
                raise ValueError('No available metrics is selected!')
            else:
                score = np.mean([metrics[key] for key in available_keys])
            scores.append(score)

        avg_score = np.nanmean(scores)
        return avg_score

    def dump_result(self, market_date: datetime.date):
        super().dump_result(market_date=market_date)

        entry_dir = pathlib.Path(self.dump_dir, f'{market_date:%Y-%m-%d}')
        grid_metrics_dir = pathlib.Path(self.dump_dir, 'GridCV')

        os.makedirs(grid_metrics_dir, exist_ok=True)

        factor_value = pd.DataFrame(self.factor_value).T

        with open(pathlib.Path(entry_dir, 'params.json'), 'w') as f:
            json.dump(
                {factor.name: {key: value for key, value in factor.params.items() if key not in ['weights']} for factor
                 in self.grid_factor}, f, indent=4)

        if os.path.isfile(selected_params_path := pathlib.Path(self.dump_dir, 'selected.csv')):
            selected_params_df = pd.read_csv(selected_params_path, index_col=0)
            selected_params_df.loc[market_date] = {
                'factor_name': self.grid_optimal_factor.name if self.grid_optimal_factor is not None else None}
            selected_params_df.to_csv(selected_params_path)
        else:
            selected_params_df = pd.DataFrame({market_date: {
                'factor_name': self.grid_optimal_factor.name if self.grid_optimal_factor is not None else None}}).T
            selected_params_df.to_csv(selected_params_path)

        pd.DataFrame(self.grid_score).to_csv(pathlib.Path(self.dump_dir, 'grid_score.csv'))

        for key in self.grid_model:
            # since the key = f'{factor_name}.{pred_var}', pred_var contains no dot in naming
            *factor_name, pred_var = key.split('.')
            factor_name = '.'.join(factor_name)
            grid_entry_dir = entry_dir.joinpath('GridCV', factor_name)

            model = self.grid_model[key]
            cv = self.grid_cv[key]
            metrics = self.grid_metrics[key]
            avg_metrics = self.grid_avg_metrics[factor_name]
            select_metrics = self.grid_select_metrics[pred_var]

            os.makedirs(grid_entry_dir, exist_ok=True)

            if cv.x_val is not None:
                fig = self._plot_cv(cv=cv, factors=factor_value)
                fig.write_html(pathlib.Path(grid_entry_dir, f'GridCV.{key}.pred.html'))
                model.dump(pathlib.Path(grid_entry_dir, f'GridCV.{key}.model.json'))
                cv.metrics.to_html(pathlib.Path(grid_entry_dir, f'GridCV.{key}.metrics.html'))

                pd.DataFrame(
                    {factor_name: value[market_date] for factor_name, value in self.grid_avg_metrics.items()}).T.to_csv(
                    pathlib.Path(grid_entry_dir.parent, f'avg_metrics.csv'))

                pd.DataFrame(metrics).T.to_csv(pathlib.Path(grid_metrics_dir, f'metrics.{key}.csv'))
                pd.DataFrame(avg_metrics).T.to_csv(pathlib.Path(grid_metrics_dir, f'avg_metrics.{factor_name}.csv'))

                pd.DataFrame(select_metrics).T.to_csv(pathlib.Path(self.dump_dir, f'select_metrics.{pred_var}.csv'))

                try:
                    path_from = pathlib.Path(self.dump_dir, f'metrics.{pred_var}.csv')
                    path_to = pathlib.Path(self.dump_dir, f'select_metrics.{pred_var}.no_train.csv')
                    if os.path.isfile(path_to):
                        os.unlink(path_to)
                    os.rename(path_from, path_to)
                except FileNotFoundError as _:
                    pass

    def reset(self):
        for factor in self.grid_factor:
            factor.clear()

        for factor in self.auxiliary_factor:
            factor.clear()

        super().reset()

        for _ in self.grid_cv.values():
            _.clear()


def main():
    """
    Main function to run factor validation or batch validation.
    """

    start_date = datetime.date(2023, 4, 1)
    end_date = datetime.date(2023, 5, 1)

    # from factor.chip import ChipAdaptiveIndexMonitor
    # factor = [
    #     ChipAdaptiveIndexMonitor(sampling_interval=5, sample_size=20, weights=INDEX_WEIGHTS, baseline_window=100,
    #                              aligned_interval=True,
    #                              name="Monitor.Grid.ChipAdaptiveMonitor"),
    #     # ChipAdaptiveIndexMonitor(sampling_interval=5, sample_size=20, weights=INDEX_WEIGHTS, baseline_window=100,
    #     #                          aligned_interval=False,
    #     #                          name="Monitor.Grid.CoherenceAdaptiveMonitor.1")
    # ]
    from factor.trade_flow import TradeFlowAdaptiveIndexMonitor
    # factor = [
    #     TradeFlowAdaptiveIndexMonitor(sampling_interval=5, sample_size=20, weights=INDEX_WEIGHTS, baseline_window=100,
    #                                   aligned_interval=True,
    #                                   name="Monitor.Grid.TradeFlowAdaptiveMonitor"),
    #     # ChipAdaptiveIndexMonitor(sampling_interval=5, sample_size=20, weights=INDEX_WEIGHTS, baseline_window=100,
    #     #                          aligned_interval=False,
    #     #                          name="Monitor.Grid.CoherenceAdaptiveMonitor.1")
    # ]

    factor = TradeFlowAdaptiveIndexMonitor(sampling_interval=5, sample_size=20, weights=INDEX_WEIGHTS,
                                           baseline_window=100,
                                           aligned_interval=True,
                                           name="Monitor.Grid.TradeFlowAdaptiveMonitor")

    # from factor.LowPass import DivergenceIndexAdaptiveMonitor
    # parent_factor = DivergenceIndexAdaptiveMonitor(weights=INDEX_WEIGHTS, sampling_interval=15, baseline_window=20)
    # auxiliary_factor = DivergenceIndexAdaptiveMonitor(weights=INDEX_WEIGHTS, sampling_interval=15, baseline_window=20)

    # from .TradeFlow.momentum import MomentumSupportIndexMonitor, MomentumSupportAdaptiveIndexMonitor
    # factor = MomentumSupportIndexMonitor(sampling_interval=15, sample_size=30, weights=INDEX_WEIGHTS)
    # factor = MomentumSupportAdaptiveIndexMonitor(sampling_interval=15, sample_size=30, baseline_window=60, aligned_interval=True, weights=INDEX_WEIGHTS)

    # validator = FactorValidation(start_date=start_date, end_date=end_date, factor=factor)

    # validator = FactorValidationCached(start_date=start_date, end_date=end_date, factor=factor)

    validator = InterTemporalValidation(start_date=start_date, end_date=end_date, factor=factor, training_days=5,
                                        override_cache=True)

    # validator = FactorParamsOptimizer(
    #     start_date=start_date,
    #     end_date=end_date,
    #     # factor=factor,
    #     # auxiliary_factor=auxiliary_factor,
    #     parent_factor=parent_factor,
    #     training_days=5,
    #     # override_cache=True
    # )

    validator.run()
    safe_exit()


if __name__ == '__main__':
    main()
